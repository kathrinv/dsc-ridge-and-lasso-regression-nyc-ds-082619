{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge and Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you've seen a number of criteria and algorithms for fitting regression models to data. You've seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You've also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using train and test splits, bias and variance, and cross validation.\n",
    "\n",
    "Now you're going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be able to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Understand what Lasso regression is \n",
    "- Understand what Ridge regression is\n",
    "- Compare and contrast Lasso and Ridge regression \n",
    "- Understand that both Lasso and Ridge regression can be used to counter multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our regression cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From previously, you know that when solving for a linear regression, you can express the cost function as\n",
    "\n",
    "$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\n",
    "\n",
    "This is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n",
    "\n",
    "$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\n",
    "\n",
    "where $k$ is the number of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalized estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've seen that when the number of predictors increases, your model complexity increases, with a higher chance of overfitting as a result. We've previously seen fairly ad-hoc variable selection methods (such as forward/backward selection), to simply select a few variables from a longer list of variables as predictors. \n",
    "\n",
    "Now, instead of completely \"deleting\" certain predictors from a model (which is equal to setting coefficients equal to zero), wouldn't it be interesting to just reduce the values of the coefficients to make them less sensitive to noise in the data? *Penalized estimation* operates in a way where parameter shrinkage effects are used to make some or \n",
    "all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. Ridge and Lasso regression are two examples of penalized estimation. There are multiple advantages to using these methods:\n",
    "\n",
    "- They reduce model complexity\n",
    "- The may prevent from overfitting\n",
    "- Some of them may perform variable selection at the same time (when coefficients are set to 0)\n",
    "- They can be used to counter multicollinearity\n",
    "\n",
    "Lasso and Ridge are two commonly used so-called **regularization techniques**. Regularization is a general term used when one tries to battle overfitting. Regularization techniques will be covered in more depth when we're moving into machine learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ridge regression, the linear regression cost function is changed by adding a penalty term to the square of the magnitude of the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have two predictors the full equation would look like this (Notice that there is a penalty term `m` for each predictor in the model - in this case, two) : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n",
    "\n",
    "$$ \\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda m_1^2 + (m_2x_{2i})-b)^2 + \\lambda m_2^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that you want to minimize your cost function, so by adding the penalty term $\\lambda$, ridge regression puts a constraint on the coefficients $m$. This means that large coefficients penalize the optimization function. That's why ridge regression leads to a shrinkage of the coefficients and helps to reduce model complexity and multicollinearity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda$ is a so-called *hyperparameter*, which means you have to specify the value for lambda. For a small lambda, the outcome of your ridge regression will resemble a linear regression model. For large lambda, penalization will increase and more parameters will shrink.\n",
    "\n",
    "Ridge regression is often also referred to as **L2 Norm Regularization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term. So, while ridge regression keeps the sum of the squared regression coefficients (except for the intercept) bounded, the lasso method bounds the sum of the absolute values.\n",
    "\n",
    "The resulting cost function looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have two predictors the full equation would look like this (Notice that there is a penalty term (m) for each predictor in the model - in this case, two) : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = $$\n",
    "\n",
    "$$\\sum_{i=1}^n(y_i - ((m_1x_{1i})-b)^2 + \\lambda \\mid m_1 \\mid) + ((m_2x_{2i})-b)^2 + \\lambda \\mid m_2 \\mid) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name \"Lasso\" comes from ‘Least Absolute Shrinkage and Selection Operator’.\n",
    "\n",
    "While looking similar to the definition of the ridge estimator, the effect of the absolute values is that some coefficients might be set exactly equal to zero, while other coefficients are shrunk towards zero. Hence the lasso method is attractive because it performs estimation *and* selection simultaneously. Especially for variable selection when the number of predictors is very high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression is often also referred to as **L1 Norm Regularization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization before Regularization\n",
    "\n",
    "An important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so if you have features that are on different scales, some will get unfairly penalized. Below, you can see that we are using a MinMaxScaler to standardize our data to the same scale. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example using our `auto-mpg` data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform our continuous predictors in `auto-mpg` and see how they perform as predictors in a Ridge versus Lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/learn-env/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(\"auto-mpg.csv\") \n",
    "data['horsepower'].astype(str).astype(int)\n",
    "y = data[[\"mpg\"]]\n",
    "X = data.drop([\"mpg\", \"car name\", \"origin\"], axis=1)\n",
    "\n",
    "scale = MinMaxScaler()\n",
    "transformed = scale.fit_transform(X)\n",
    "X = pd.DataFrame(transformed, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.617571</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.536150</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.728682</td>\n",
       "      <td>0.646739</td>\n",
       "      <td>0.589736</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.645995</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.516870</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.609819</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.516019</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>0.510870</td>\n",
       "      <td>0.520556</td>\n",
       "      <td>0.148810</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.932817</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.773462</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.997416</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.777148</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.961240</td>\n",
       "      <td>0.918478</td>\n",
       "      <td>0.765240</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972826</td>\n",
       "      <td>0.797278</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.832041</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.634250</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.813953</td>\n",
       "      <td>0.673913</td>\n",
       "      <td>0.552878</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.702842</td>\n",
       "      <td>0.619565</td>\n",
       "      <td>0.565920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.857881</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.609016</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972826</td>\n",
       "      <td>0.417635</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.266304</td>\n",
       "      <td>0.215197</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.335917</td>\n",
       "      <td>0.266304</td>\n",
       "      <td>0.345903</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.338501</td>\n",
       "      <td>0.277174</td>\n",
       "      <td>0.329175</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.341085</td>\n",
       "      <td>0.211957</td>\n",
       "      <td>0.276155</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.074935</td>\n",
       "      <td>0.228261</td>\n",
       "      <td>0.146583</td>\n",
       "      <td>0.386905</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.074935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062943</td>\n",
       "      <td>0.744048</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.108527</td>\n",
       "      <td>0.222826</td>\n",
       "      <td>0.300255</td>\n",
       "      <td>0.565476</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.100775</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.231642</td>\n",
       "      <td>0.386905</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.266304</td>\n",
       "      <td>0.216048</td>\n",
       "      <td>0.565476</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.136951</td>\n",
       "      <td>0.364130</td>\n",
       "      <td>0.176070</td>\n",
       "      <td>0.267857</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.338501</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.293451</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.754522</td>\n",
       "      <td>0.918478</td>\n",
       "      <td>0.851148</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.617571</td>\n",
       "      <td>0.836957</td>\n",
       "      <td>0.783385</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.645995</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.785086</td>\n",
       "      <td>0.327381</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.609819</td>\n",
       "      <td>0.798913</td>\n",
       "      <td>0.884321</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.074935</td>\n",
       "      <td>0.228261</td>\n",
       "      <td>0.146583</td>\n",
       "      <td>0.386905</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.113695</td>\n",
       "      <td>0.228261</td>\n",
       "      <td>0.281259</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.113695</td>\n",
       "      <td>0.228261</td>\n",
       "      <td>0.291182</td>\n",
       "      <td>0.630952</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.113695</td>\n",
       "      <td>0.228261</td>\n",
       "      <td>0.221718</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.113695</td>\n",
       "      <td>0.211957</td>\n",
       "      <td>0.272753</td>\n",
       "      <td>0.488095</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.173127</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.258577</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.214470</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.318117</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.354976</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.095607</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>0.104054</td>\n",
       "      <td>0.434524</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.059432</td>\n",
       "      <td>0.119565</td>\n",
       "      <td>0.116813</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.059432</td>\n",
       "      <td>0.119565</td>\n",
       "      <td>0.101219</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.095607</td>\n",
       "      <td>0.092391</td>\n",
       "      <td>0.145166</td>\n",
       "      <td>0.398810</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.077519</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.145166</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.134367</td>\n",
       "      <td>0.228261</td>\n",
       "      <td>0.155089</td>\n",
       "      <td>0.386905</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.100775</td>\n",
       "      <td>0.157609</td>\n",
       "      <td>0.167848</td>\n",
       "      <td>0.386905</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.103359</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.179189</td>\n",
       "      <td>0.529762</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.059432</td>\n",
       "      <td>0.114130</td>\n",
       "      <td>0.099802</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.059432</td>\n",
       "      <td>0.114130</td>\n",
       "      <td>0.099802</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.059432</td>\n",
       "      <td>0.114130</td>\n",
       "      <td>0.108307</td>\n",
       "      <td>0.488095</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.291990</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.377658</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.501292</td>\n",
       "      <td>0.211957</td>\n",
       "      <td>0.397505</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.227390</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.275588</td>\n",
       "      <td>0.386905</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.423773</td>\n",
       "      <td>0.358696</td>\n",
       "      <td>0.346470</td>\n",
       "      <td>0.398810</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.196382</td>\n",
       "      <td>0.271739</td>\n",
       "      <td>0.298270</td>\n",
       "      <td>0.351190</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.173127</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.214630</td>\n",
       "      <td>0.297619</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.214470</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.379076</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.333711</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.074935</td>\n",
       "      <td>0.032609</td>\n",
       "      <td>0.146583</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.173127</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.193365</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.134367</td>\n",
       "      <td>0.179348</td>\n",
       "      <td>0.286929</td>\n",
       "      <td>0.630952</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.131783</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.313864</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>392 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cylinders  displacement  horsepower    weight  acceleration  model year\n",
       "0          1.0      0.617571    0.456522  0.536150      0.238095    0.000000\n",
       "1          1.0      0.728682    0.646739  0.589736      0.208333    0.000000\n",
       "2          1.0      0.645995    0.565217  0.516870      0.178571    0.000000\n",
       "3          1.0      0.609819    0.565217  0.516019      0.238095    0.000000\n",
       "4          1.0      0.604651    0.510870  0.520556      0.148810    0.000000\n",
       "5          1.0      0.932817    0.826087  0.773462      0.119048    0.000000\n",
       "6          1.0      0.997416    0.945652  0.777148      0.059524    0.000000\n",
       "7          1.0      0.961240    0.918478  0.765240      0.029762    0.000000\n",
       "8          1.0      1.000000    0.972826  0.797278      0.119048    0.000000\n",
       "9          1.0      0.832041    0.782609  0.634250      0.029762    0.000000\n",
       "10         1.0      0.813953    0.673913  0.552878      0.119048    0.000000\n",
       "11         1.0      0.702842    0.619565  0.565920      0.000000    0.000000\n",
       "12         1.0      0.857881    0.565217  0.609016      0.089286    0.000000\n",
       "13         1.0      1.000000    0.972826  0.417635      0.119048    0.000000\n",
       "14         0.2      0.116279    0.266304  0.215197      0.416667    0.000000\n",
       "15         0.6      0.335917    0.266304  0.345903      0.446429    0.000000\n",
       "16         0.6      0.338501    0.277174  0.329175      0.446429    0.000000\n",
       "17         0.6      0.341085    0.211957  0.276155      0.476190    0.000000\n",
       "18         0.2      0.074935    0.228261  0.146583      0.386905    0.000000\n",
       "19         0.2      0.074935    0.000000  0.062943      0.744048    0.000000\n",
       "20         0.2      0.108527    0.222826  0.300255      0.565476    0.000000\n",
       "21         0.2      0.100775    0.239130  0.231642      0.386905    0.000000\n",
       "22         0.2      0.093023    0.266304  0.216048      0.565476    0.000000\n",
       "23         0.2      0.136951    0.364130  0.176070      0.267857    0.000000\n",
       "24         0.6      0.338501    0.239130  0.293451      0.416667    0.000000\n",
       "25         1.0      0.754522    0.918478  0.851148      0.357143    0.000000\n",
       "26         1.0      0.617571    0.836957  0.783385      0.416667    0.000000\n",
       "27         1.0      0.645995    0.891304  0.785086      0.327381    0.000000\n",
       "28         1.0      0.609819    0.798913  0.884321      0.625000    0.000000\n",
       "29         0.2      0.074935    0.228261  0.146583      0.386905    0.083333\n",
       "..         ...           ...         ...       ...           ...         ...\n",
       "362        0.2      0.113695    0.228261  0.281259      0.690476    1.000000\n",
       "363        0.2      0.113695    0.228261  0.291182      0.630952    1.000000\n",
       "364        0.2      0.113695    0.228261  0.221718      0.595238    1.000000\n",
       "365        0.2      0.113695    0.211957  0.272753      0.488095    1.000000\n",
       "366        0.2      0.173127    0.206522  0.258577      0.476190    1.000000\n",
       "367        0.2      0.214470    0.239130  0.318117      0.595238    1.000000\n",
       "368        0.2      0.186047    0.250000  0.354976      0.500000    1.000000\n",
       "369        0.2      0.095607    0.152174  0.104054      0.434524    1.000000\n",
       "370        0.2      0.059432    0.119565  0.116813      0.607143    1.000000\n",
       "371        0.2      0.059432    0.119565  0.101219      0.571429    1.000000\n",
       "372        0.2      0.095607    0.092391  0.145166      0.398810    1.000000\n",
       "373        0.2      0.077519    0.130435  0.145166      0.553571    1.000000\n",
       "374        0.2      0.134367    0.228261  0.155089      0.386905    1.000000\n",
       "375        0.2      0.100775    0.157609  0.167848      0.386905    1.000000\n",
       "376        0.2      0.103359    0.130435  0.179189      0.529762    1.000000\n",
       "377        0.2      0.059432    0.114130  0.099802      0.416667    1.000000\n",
       "378        0.2      0.059432    0.114130  0.099802      0.458333    1.000000\n",
       "379        0.2      0.059432    0.114130  0.108307      0.488095    1.000000\n",
       "380        0.6      0.291990    0.347826  0.377658      0.500000    1.000000\n",
       "381        0.6      0.501292    0.211957  0.397505      0.535714    1.000000\n",
       "382        0.2      0.227390    0.250000  0.275588      0.386905    1.000000\n",
       "383        0.6      0.423773    0.358696  0.346470      0.398810    1.000000\n",
       "384        0.2      0.196382    0.271739  0.298270      0.351190    1.000000\n",
       "385        0.2      0.173127    0.206522  0.214630      0.297619    1.000000\n",
       "386        0.2      0.214470    0.239130  0.379076      0.553571    1.000000\n",
       "387        0.2      0.186047    0.217391  0.333711      0.452381    1.000000\n",
       "388        0.2      0.074935    0.032609  0.146583      0.988095    1.000000\n",
       "389        0.2      0.173127    0.206522  0.193365      0.214286    1.000000\n",
       "390        0.2      0.134367    0.179348  0.286929      0.630952    1.000000\n",
       "391        0.2      0.131783    0.195652  0.313864      0.678571    1.000000\n",
       "\n",
       "[392 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we've performed train-test-splits and fit Ridge, Lasso and Linear regression models. Notice that the Ridge and Lasso models have the parameter alpha, which is Scikit's version of $\\lambda$ in the regularization cost functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform test train split\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n",
    "\n",
    "# Build a Ridge, Lasso and regular linear regression model. \n",
    "# Note how in scikit learn, the regularization parameter is denoted by alpha (and not lambda)\n",
    "ridge = Ridge(alpha=0.5)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "lasso = Lasso(alpha=0.5)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "lin = LinearRegression()\n",
    "lin.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create predictions for train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preditions for training and test sets\n",
    "y_h_ridge_train = ridge.predict(X_train)\n",
    "y_h_ridge_test = ridge.predict(X_test)\n",
    "\n",
    "y_h_lasso_train = np.reshape(lasso.predict(X_train), (274,1))\n",
    "y_h_lasso_test = np.reshape(lasso.predict(X_test), (118,1))\n",
    "\n",
    "y_h_lin_train = lin.predict(X_train)\n",
    "y_h_lin_test = lin.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the RSS for train and test for each of the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error Ridge Model mpg    2688.222824\n",
      "dtype: float64\n",
      "Test Error Ridge Model mpg    2074.197775\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Train Error Lasso Model mpg    4644.536425\n",
      "dtype: float64\n",
      "Test Error Lasso Model mpg    3696.183375\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Train Error Unpenalized Linear Model mpg    2658.043444\n",
      "dtype: float64\n",
      "Test Error Unpenalized Linear Model mpg    1976.266987\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Train Error Ridge Model', np.sum((y_train - y_h_ridge_train)**2))\n",
    "print('Test Error Ridge Model', np.sum((y_test - y_h_ridge_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Lasso Model', np.sum((y_train - y_h_lasso_train)**2))\n",
    "print('Test Error Lasso Model', np.sum((y_test - y_h_lasso_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Unpenalized Linear Model', np.sum((y_train - lin.predict(X_train))**2))\n",
    "print('Test Error Unpenalized Linear Model', np.sum((y_test - lin.predict(X_test))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let's see how including Ridge and Lasso changed our parameter estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge parameter coefficients: [[ -2.11792413  -3.0112953   -1.90579654 -15.60758962  -1.61071692\n",
      "    8.12940111]]\n",
      "Lasso parameter coefficients: [-10.31005725  -0.          -0.          -2.27967948   0.\n",
      "   3.88327477]\n",
      "Linear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -20.08143923  -0.39639115\n",
      "    8.56051229]]\n"
     ]
    }
   ],
   "source": [
    "print('Ridge parameter coefficients:', ridge.coef_)\n",
    "print('Lasso parameter coefficients:', lasso.coef_)\n",
    "print('Linear model parameter coefficients:', lin.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see how Lasso shrinks certain parameters to 0! The Ridge regression mostly affected the fourth parameter (estimated to be -20.08 for the linear regression model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional reading\n",
    "\n",
    "Full code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found [here](https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/).\n",
    "\n",
    "Make sure to have a look at the Scikit-Learn documentation for [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) and [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You now know how to perform Lasso and Ridge regression. Let's move on to the lab to explore Lasso and Ridge further!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
